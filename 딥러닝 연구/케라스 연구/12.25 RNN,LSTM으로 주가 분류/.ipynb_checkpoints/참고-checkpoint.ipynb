{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2190.08 2203.71 2204.18 2196.56 2194.76 2195.68 2168.15 2170.25 2137.35\n",
      " 2105.62 2098.   2088.65 2081.85 2060.74 2068.89 2084.07 2091.92 2087.96\n",
      " 2118.6  2127.85 2121.35 2123.5  2101.96 2096.6  2125.32 2153.24 2160.69\n",
      " 2162.18 2139.23 2122.45 2140.92 2124.09 2137.23 2144.29 2144.15 2142.64\n",
      " 2130.24 2100.2  2083.48 2080.27 2092.69 2093.6  2087.89 2085.66 2080.62\n",
      " 2088.86 2064.84 2060.69 2077.94 2082.83 2068.17 2067.4  2044.61 2028.15\n",
      " 2046.25 2021.73 2020.69 2031.91 2072.42 2063.05 2049.93 2074.52 2073.39\n",
      " 2101.04 2091.7  2091.52 2080.35 2070.73 2062.33 2062.22 2049.2  2032.08\n",
      " 2019.55 2009.13 2004.75 1988.53 1965.69 1969.19 1967.79 1933.41 1941.09\n",
      " 1924.6  1916.31 1948.3  1951.01 1964.65 1960.25 1939.9  1927.17 1938.37\n",
      " 1925.83 1942.29 1937.75 1920.61 1909.71 1917.5  1946.98 1998.13 2017.34\n",
      " 2024.55 2038.68 2029.48 2066.26 2074.48 2082.3  2101.45 2093.34 2094.36\n",
      " 2066.55 2072.92 2091.87 2082.48 2086.66 2080.58 2058.78 2052.03 2064.17\n",
      " 2110.59 2108.73 2096.02 2122.02 2129.74 2130.62 2134.32 2121.85 2121.64\n",
      " 2126.33 2125.62 2131.29 2124.78 2098.71 2090.73 2095.41 2103.15 2108.75\n",
      " 2111.81 2099.49 2072.33 2069.11 2066.97 2067.85 2041.74 2038.8  2023.32\n",
      " 2048.83 2044.21 2045.31 2059.59 2064.86 2061.25 2055.71 2055.8  2067.69\n",
      " 2092.78 2081.84 2079.01 2108.04 2102.01 2168.01 2176.99 2196.32 2212.75\n",
      " 2203.59 2216.43 2179.31 2190.5  2201.03 2220.51 2216.65 2216.15 2213.77\n",
      " 2245.89 2248.63 2242.88 2233.45 2224.44 2224.39 2213.56 2210.6  2209.61\n",
      " 2206.53 2203.27 2177.18 2168.28 2140.67 2128.1  2145.62 2148.8  2144.86\n",
      " 2186.95 2184.88 2177.1  2177.62 2179.49 2176.11 2155.68 2148.41 2157.18\n",
      " 2138.1  2137.44 2165.79 2175.6  2179.23 2190.66 2195.44 2234.79 2226.6\n",
      " 2232.56 2230.5  2228.66 2229.76 2205.63 2210.89 2196.09 2225.85 2201.48\n",
      " 2190.47 2180.73 2177.05 2203.42 2203.46 2204.85 2206.2  2183.36 2177.3\n",
      " 2177.73 2145.03 2127.78 2117.77 2124.61 2124.28 2107.06 2106.1  2097.18\n",
      " 2064.52 2075.57 2063.28 2064.71 2025.27 2037.1  2010.25 1993.7  2010.\n",
      " 2041.04 2028.44 2028.01 2055.01 2061.49 2060.12 2078.84 2062.11 2071.09\n",
      " 2069.38 2095.55 2082.57 2052.97 2053.79 2075.76 2068.69 2101.31 2114.35\n",
      " 2131.93 2096.86 2114.1  2108.22 2099.42 2083.02 2057.48 2069.95 2076.55\n",
      " 2082.58 2100.56 2092.4  2088.06 2068.05 2071.23 2080.44 2086.09 2092.63\n",
      " 2078.69 2089.62 2076.92 2096.   2024.46 2029.69 2014.69 1996.05 2027.15\n",
      " 2063.3  2097.58 2106.1  2161.71 2156.26 2148.31 2167.51 2145.12 2145.12\n",
      " 2161.85 2129.67 2228.61 2253.83 2267.52 2274.49 2309.57 2338.88 2343.07\n",
      " 2355.43 2339.17 2323.45 2308.46 2308.98 2303.01 2318.25 2286.23 2282.92\n",
      " 2283.2  2288.66 2281.58 2287.61 2291.77 2315.72 2307.03 2322.88 2307.35\n",
      " 2309.03 2303.12 2299.3  2293.21 2282.6  2273.33 2270.06 2247.88 2247.05\n",
      " 2240.8  2258.91 2248.45 2282.79 2303.71 2301.45 2300.16 2286.5  2287.68\n",
      " 2270.2  2307.07 2295.26 2293.51 2294.99 2289.06 2273.03 2280.2  2269.31\n",
      " 2289.19 2282.29 2290.11 2297.92 2301.99 2310.9  2285.06 2280.62 2294.16\n",
      " 2285.8  2272.87 2257.55 2265.46 2272.76 2271.54 2326.13 2314.24 2342.03\n",
      " 2350.92 2357.88 2357.22 2337.83 2363.91 2340.11 2376.24 2404.04 2423.48\n",
      " 2468.83 2470.15 2451.58 2470.58 2453.76 2447.76 2438.96 2423.01 2409.03\n",
      " 2457.25 2478.96 2460.8  2466.01 2471.91 2465.57 2460.65 2448.45 2459.82\n",
      " 2458.54 2476.11 2477.71 2464.16 2443.98 2449.81 2461.38 2487.25 2505.61\n",
      " 2515.38 2492.4  2475.64 2448.81 2464.14 2474.11 2476.33 2486.1  2479.98\n",
      " 2453.77 2457.49 2455.07 2442.71 2444.22 2450.74 2444.08 2429.58 2437.52\n",
      " 2408.06 2442.43 2444.16 2445.85 2436.37 2419.29 2452.06 2437.08 2416.76\n",
      " 2496.02 2484.97 2485.52 2475.03 2493.97 2492.38 2486.08 2494.49 2484.12\n",
      " 2459.45 2433.08 2401.82 2411.41 2375.06 2402.16 2427.36 2456.14 2457.65\n",
      " 2451.52 2414.28 2429.65 2415.12 2442.82 2421.83 2395.19 2385.38 2363.77\n",
      " 2407.62 2396.56 2453.31 2491.75 2525.39 2568.54 2566.46 2567.74 2598.19\n",
      " 2574.76 2562.23 2538.   2536.6  2502.11 2520.26 2515.81 2515.43 2521.74\n",
      " 2503.73 2496.42 2487.91 2499.75 2510.23 2513.28 2497.52 2466.46 2486.35\n",
      " 2479.65]\n",
      "438 49\n",
      "Epoch 1/10\n",
      " - 1s - loss: 4796036.3521\n",
      "Epoch 2/10\n",
      " - 1s - loss: 4794133.9885\n",
      "Epoch 3/10\n",
      " - 1s - loss: 4792231.7248\n",
      "Epoch 4/10\n",
      " - 1s - loss: 4790332.1491\n",
      "Epoch 5/10\n",
      " - 1s - loss: 4788432.5992\n",
      "Epoch 6/10\n",
      " - 1s - loss: 4786533.7104\n",
      "Epoch 7/10\n",
      " - 1s - loss: 4784635.7569\n",
      "Epoch 8/10\n",
      " - 1s - loss: 4782738.3028\n",
      "Epoch 9/10\n",
      " - 1s - loss: 4780840.5745\n",
      "Epoch 10/10\n",
      " - 1s - loss: 4778942.3784\n",
      "47/47 [==============================] - 0s 4ms/step\n",
      "\n",
      " 테스트 정확도 : 6109865.8511\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import FinanceDataReader as fdr\n",
    "look_back = 1\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i + look_back)]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    " \n",
    "# file loader\n",
    "sydtpath = \"D:sydt\"\n",
    "naturalEndoTekCode = \"A168330\"\n",
    "fullpath = sydtpath + os.path.sep + naturalEndoTekCode + '.csv'\n",
    "pandf = fdr.DataReader(\"KS11\",\"2018\")\n",
    "# convert nparray\n",
    "nparr = pandf['Close'].values[::-1]\n",
    "nparr.astype('float32')\n",
    "print(nparr)\n",
    "\n",
    "nptf = nparr\n",
    " \n",
    "# split train, test\n",
    "train_size = int(len(nptf) * 0.9)\n",
    "test_size = len(nptf) - train_size\n",
    "train, test = nptf[0:train_size], nptf[train_size:len(nptf)]\n",
    "print(len(train), len(test))\n",
    " \n",
    "# create dataset for learning\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    " \n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    " \n",
    "# simple lstm network learning\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=10, batch_size=1, verbose=2)\n",
    "\n",
    "print(\"\\n 테스트 정확도 : %.4f\" % (model.evaluate(testX,testY)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
