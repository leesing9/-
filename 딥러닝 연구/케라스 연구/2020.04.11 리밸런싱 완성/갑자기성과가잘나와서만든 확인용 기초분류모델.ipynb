{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import SimpleRNN\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import FinanceDataReader as fdr\n",
    "#특성 추가 ------------------------------------------\n",
    "    #이동평균선\n",
    "def get_MA(df):\n",
    "    MA_26=df[\"Close\"].rolling(26).mean()\n",
    "    MA_52=df[\"Close\"].rolling(52).mean()\n",
    "    df=df.assign(MA_26=MA_26,MA_52=MA_52).dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "    #스토캐스틱\n",
    "def get_stochastic(df, n=15, m=5, t=3):\n",
    "    # n일중 최고가\n",
    "    ndays_high = df.High.rolling(window=n, min_periods=1).max()\n",
    "    # n일중 최저가\n",
    "    ndays_low = df.Low.rolling(window=n, min_periods=1).min()\n",
    " \n",
    "    # Fast%K 계산\n",
    "    kdj_k = ((df.Close - ndays_low) / (ndays_high - ndays_low))*100\n",
    "    # Fast%D (=Slow%K) 계산\n",
    "    kdj_d = kdj_k.ewm(span=m).mean()\n",
    "    # Slow%D 계산\n",
    "    kdj_j = kdj_d.ewm(span=t).mean()\n",
    " \n",
    "    # dataframe에 컬럼 추가\n",
    "    df = df.assign(kdj_k=kdj_k, kdj_d=kdj_d, kdj_j=kdj_j).dropna()\n",
    "    \n",
    "    return df\n",
    "   \n",
    "    #시간\n",
    "def get_time(df):\n",
    "    time=np.linspace(0,10,len(df),endpoint=False).reshape(-1,1)\n",
    "    df=df.assign(time=time)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Program Files\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Program Files\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Program Files\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 66 samples, validate on 8 samples\n",
      "Epoch 1/100\n",
      "66/66 [==============================] - 0s 6ms/step - loss: 0.7694 - acc: 0.4394 - val_loss: 0.7147 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "66/66 [==============================] - 0s 558us/step - loss: 0.7351 - acc: 0.4394 - val_loss: 0.7046 - val_acc: 0.5000\n",
      "Epoch 3/100\n",
      "66/66 [==============================] - 0s 574us/step - loss: 0.7190 - acc: 0.4394 - val_loss: 0.6989 - val_acc: 0.5000\n",
      "Epoch 4/100\n",
      "66/66 [==============================] - 0s 756us/step - loss: 0.7068 - acc: 0.4394 - val_loss: 0.6937 - val_acc: 0.5000\n",
      "Epoch 5/100\n",
      "66/66 [==============================] - 0s 816us/step - loss: 0.6969 - acc: 0.4394 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 6/100\n",
      "66/66 [==============================] - 0s 544us/step - loss: 0.6914 - acc: 0.5606 - val_loss: 0.6940 - val_acc: 0.5000\n",
      "Epoch 7/100\n",
      "66/66 [==============================] - 0s 544us/step - loss: 0.6883 - acc: 0.5606 - val_loss: 0.6963 - val_acc: 0.5000\n",
      "Epoch 8/100\n",
      "66/66 [==============================] - 0s 892us/step - loss: 0.6864 - acc: 0.5606 - val_loss: 0.7000 - val_acc: 0.5000\n",
      "Epoch 9/100\n",
      "66/66 [==============================] - 0s 544us/step - loss: 0.6857 - acc: 0.5606 - val_loss: 0.7001 - val_acc: 0.5000\n",
      "Epoch 10/100\n",
      "66/66 [==============================] - 0s 559us/step - loss: 0.6856 - acc: 0.5606 - val_loss: 0.6989 - val_acc: 0.5000\n",
      "Epoch 11/100\n",
      "66/66 [==============================] - 0s 604us/step - loss: 0.6857 - acc: 0.5606 - val_loss: 0.6968 - val_acc: 0.5000\n",
      "Epoch 12/100\n",
      "66/66 [==============================] - 0s 559us/step - loss: 0.6858 - acc: 0.5606 - val_loss: 0.6972 - val_acc: 0.5000\n",
      "Epoch 13/100\n",
      "66/66 [==============================] - 0s 544us/step - loss: 0.6855 - acc: 0.5606 - val_loss: 0.6985 - val_acc: 0.5000\n",
      "Epoch 14/100\n",
      "66/66 [==============================] - 0s 589us/step - loss: 0.6849 - acc: 0.5606 - val_loss: 0.7017 - val_acc: 0.5000\n",
      "Epoch 15/100\n",
      "66/66 [==============================] - 0s 635us/step - loss: 0.6857 - acc: 0.5606 - val_loss: 0.7022 - val_acc: 0.5000\n",
      "Epoch 16/100\n",
      "66/66 [==============================] - 0s 604us/step - loss: 0.6855 - acc: 0.5606 - val_loss: 0.6983 - val_acc: 0.5000\n",
      "Epoch 17/100\n",
      "66/66 [==============================] - 0s 589us/step - loss: 0.6844 - acc: 0.5606 - val_loss: 0.6974 - val_acc: 0.5000\n",
      "Epoch 18/100\n",
      "66/66 [==============================] - 0s 801us/step - loss: 0.6845 - acc: 0.5606 - val_loss: 0.6999 - val_acc: 0.5000\n",
      "Epoch 19/100\n",
      "66/66 [==============================] - 0s 801us/step - loss: 0.6839 - acc: 0.5606 - val_loss: 0.6981 - val_acc: 0.5000\n",
      "Epoch 20/100\n",
      "66/66 [==============================] - 0s 846us/step - loss: 0.6831 - acc: 0.5606 - val_loss: 0.6948 - val_acc: 0.5000\n",
      "Epoch 21/100\n",
      "66/66 [==============================] - 0s 589us/step - loss: 0.6859 - acc: 0.5606 - val_loss: 0.6926 - val_acc: 0.5000\n",
      "Epoch 22/100\n",
      "66/66 [==============================] - 0s 544us/step - loss: 0.6841 - acc: 0.5606 - val_loss: 0.6964 - val_acc: 0.5000\n",
      "Epoch 23/100\n",
      "66/66 [==============================] - 0s 559us/step - loss: 0.6823 - acc: 0.5606 - val_loss: 0.6996 - val_acc: 0.5000\n",
      "Epoch 24/100\n",
      "66/66 [==============================] - 0s 574us/step - loss: 0.6829 - acc: 0.5606 - val_loss: 0.7053 - val_acc: 0.5000\n",
      "Epoch 25/100\n",
      "66/66 [==============================] - 0s 816us/step - loss: 0.6814 - acc: 0.5606 - val_loss: 0.7031 - val_acc: 0.5000\n",
      "Epoch 26/100\n",
      "66/66 [==============================] - 0s 544us/step - loss: 0.6806 - acc: 0.5606 - val_loss: 0.7091 - val_acc: 0.5000\n",
      "Epoch 27/100\n",
      "66/66 [==============================] - 0s 559us/step - loss: 0.6794 - acc: 0.5606 - val_loss: 0.7066 - val_acc: 0.5000\n",
      "Epoch 28/100\n",
      "66/66 [==============================] - 0s 635us/step - loss: 0.6773 - acc: 0.5606 - val_loss: 0.7035 - val_acc: 0.5000\n",
      "Epoch 29/100\n",
      "66/66 [==============================] - 0s 574us/step - loss: 0.6755 - acc: 0.5606 - val_loss: 0.7077 - val_acc: 0.5000\n",
      "Epoch 30/100\n",
      "66/66 [==============================] - 0s 816us/step - loss: 0.6733 - acc: 0.5606 - val_loss: 0.7118 - val_acc: 0.5000\n",
      "Epoch 31/100\n",
      "66/66 [==============================] - 0s 574us/step - loss: 0.6710 - acc: 0.5606 - val_loss: 0.7058 - val_acc: 0.5000\n",
      "Epoch 32/100\n",
      "66/66 [==============================] - 0s 604us/step - loss: 0.6675 - acc: 0.5606 - val_loss: 0.7053 - val_acc: 0.5000\n",
      "Epoch 33/100\n",
      "66/66 [==============================] - 0s 559us/step - loss: 0.6652 - acc: 0.5606 - val_loss: 0.7067 - val_acc: 0.5000\n",
      "Epoch 34/100\n",
      "66/66 [==============================] - 0s 620us/step - loss: 0.6600 - acc: 0.5606 - val_loss: 0.7062 - val_acc: 0.5000\n",
      "Epoch 35/100\n",
      "66/66 [==============================] - 0s 559us/step - loss: 0.6560 - acc: 0.5606 - val_loss: 0.7108 - val_acc: 0.5000\n",
      "Epoch 36/100\n",
      "66/66 [==============================] - 0s 604us/step - loss: 0.6617 - acc: 0.5606 - val_loss: 0.6800 - val_acc: 0.5000\n",
      "Epoch 37/100\n",
      "66/66 [==============================] - 0s 846us/step - loss: 0.6564 - acc: 0.7424 - val_loss: 0.6707 - val_acc: 0.6250\n",
      "Epoch 38/100\n",
      "66/66 [==============================] - 0s 635us/step - loss: 0.6401 - acc: 0.7879 - val_loss: 0.6855 - val_acc: 0.5000\n",
      "Epoch 39/100\n",
      "66/66 [==============================] - 0s 831us/step - loss: 0.6410 - acc: 0.7576 - val_loss: 0.6961 - val_acc: 0.5000\n",
      "Epoch 40/100\n",
      "66/66 [==============================] - 0s 665us/step - loss: 0.6414 - acc: 0.7576 - val_loss: 0.7356 - val_acc: 0.3750\n",
      "Epoch 41/100\n",
      "66/66 [==============================] - 0s 589us/step - loss: 0.6215 - acc: 0.7424 - val_loss: 0.7212 - val_acc: 0.5000\n",
      "Epoch 42/100\n",
      "66/66 [==============================] - 0s 589us/step - loss: 0.6094 - acc: 0.8030 - val_loss: 0.6995 - val_acc: 0.5000\n",
      "Epoch 43/100\n",
      "66/66 [==============================] - 0s 695us/step - loss: 0.6001 - acc: 0.7879 - val_loss: 0.6720 - val_acc: 0.6250\n",
      "Epoch 44/100\n",
      "66/66 [==============================] - 0s 710us/step - loss: 0.5874 - acc: 0.8485 - val_loss: 0.6658 - val_acc: 0.6250\n",
      "Epoch 45/100\n",
      "66/66 [==============================] - 0s 635us/step - loss: 0.5768 - acc: 0.8182 - val_loss: 0.6723 - val_acc: 0.6250\n",
      "Epoch 46/100\n",
      "66/66 [==============================] - 0s 952us/step - loss: 0.5643 - acc: 0.8333 - val_loss: 0.6803 - val_acc: 0.6250\n",
      "Epoch 47/100\n",
      "66/66 [==============================] - 0s 665us/step - loss: 0.5559 - acc: 0.8182 - val_loss: 0.7304 - val_acc: 0.5000\n",
      "Epoch 48/100\n",
      "66/66 [==============================] - 0s 665us/step - loss: 0.5425 - acc: 0.8333 - val_loss: 0.7561 - val_acc: 0.3750\n",
      "Epoch 49/100\n",
      "66/66 [==============================] - 0s 786us/step - loss: 0.5372 - acc: 0.7879 - val_loss: 0.7701 - val_acc: 0.5000\n",
      "Epoch 50/100\n",
      "66/66 [==============================] - 0s 665us/step - loss: 0.5166 - acc: 0.8333 - val_loss: 0.7158 - val_acc: 0.5000\n",
      "Epoch 51/100\n",
      "66/66 [==============================] - 0s 604us/step - loss: 0.5140 - acc: 0.8030 - val_loss: 0.5966 - val_acc: 0.7500\n",
      "Epoch 52/100\n",
      "66/66 [==============================] - 0s 619us/step - loss: 0.5228 - acc: 0.7576 - val_loss: 0.5768 - val_acc: 0.7500\n",
      "Epoch 53/100\n",
      "66/66 [==============================] - 0s 620us/step - loss: 0.5280 - acc: 0.7727 - val_loss: 0.5696 - val_acc: 0.7500\n",
      "Epoch 54/100\n",
      "66/66 [==============================] - 0s 589us/step - loss: 0.5530 - acc: 0.7424 - val_loss: 0.5672 - val_acc: 0.7500\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 0s 816us/step - loss: 0.4792 - acc: 0.8333 - val_loss: 0.5736 - val_acc: 0.7500\n",
      "Epoch 56/100\n",
      "66/66 [==============================] - 0s 574us/step - loss: 0.5462 - acc: 0.7273 - val_loss: 0.5811 - val_acc: 0.8750\n",
      "Epoch 57/100\n",
      "66/66 [==============================] - 0s 529us/step - loss: 0.5538 - acc: 0.7121 - val_loss: 0.5625 - val_acc: 0.7500\n",
      "Epoch 58/100\n",
      "66/66 [==============================] - 0s 499us/step - loss: 0.4611 - acc: 0.8636 - val_loss: 0.5553 - val_acc: 0.7500\n",
      "Epoch 59/100\n",
      "66/66 [==============================] - 0s 786us/step - loss: 0.5004 - acc: 0.8182 - val_loss: 0.5493 - val_acc: 0.7500\n",
      "Epoch 60/100\n",
      "66/66 [==============================] - 0s 468us/step - loss: 0.4814 - acc: 0.8182 - val_loss: 0.5405 - val_acc: 0.7500\n",
      "Epoch 61/100\n",
      "66/66 [==============================] - 0s 544us/step - loss: 0.4210 - acc: 0.8939 - val_loss: 0.5247 - val_acc: 0.7500\n",
      "Epoch 62/100\n",
      "66/66 [==============================] - 0s 786us/step - loss: 0.4282 - acc: 0.8333 - val_loss: 0.5205 - val_acc: 0.7500\n",
      "Epoch 63/100\n",
      "66/66 [==============================] - 0s 468us/step - loss: 0.4111 - acc: 0.8333 - val_loss: 0.5146 - val_acc: 0.7500\n",
      "Epoch 64/100\n",
      "66/66 [==============================] - 0s 468us/step - loss: 0.4030 - acc: 0.8788 - val_loss: 0.5114 - val_acc: 0.8750\n",
      "Epoch 65/100\n",
      "66/66 [==============================] - 0s 484us/step - loss: 0.4002 - acc: 0.8788 - val_loss: 0.4873 - val_acc: 0.8750\n",
      "Epoch 66/100\n",
      "66/66 [==============================] - 0s 529us/step - loss: 0.3706 - acc: 0.8636 - val_loss: 0.4960 - val_acc: 0.7500\n",
      "Epoch 67/100\n",
      "66/66 [==============================] - 0s 544us/step - loss: 0.3678 - acc: 0.8636 - val_loss: 0.4824 - val_acc: 0.7500\n",
      "Epoch 68/100\n",
      "66/66 [==============================] - 0s 499us/step - loss: 0.3519 - acc: 0.8788 - val_loss: 0.4570 - val_acc: 0.7500\n",
      "Epoch 69/100\n",
      "66/66 [==============================] - 0s 499us/step - loss: 0.3386 - acc: 0.8788 - val_loss: 0.4319 - val_acc: 0.7500\n",
      "Epoch 70/100\n",
      "66/66 [==============================] - 0s 484us/step - loss: 0.3390 - acc: 0.8939 - val_loss: 0.4275 - val_acc: 0.7500\n",
      "Epoch 71/100\n",
      "66/66 [==============================] - 0s 756us/step - loss: 0.3432 - acc: 0.8939 - val_loss: 0.3984 - val_acc: 0.8750\n",
      "Epoch 72/100\n",
      "66/66 [==============================] - 0s 514us/step - loss: 0.3527 - acc: 0.8636 - val_loss: 0.3946 - val_acc: 0.7500\n",
      "Epoch 73/100\n",
      "66/66 [==============================] - 0s 514us/step - loss: 0.3033 - acc: 0.9091 - val_loss: 0.4062 - val_acc: 0.8750\n",
      "Epoch 74/100\n",
      "66/66 [==============================] - 0s 514us/step - loss: 0.3524 - acc: 0.8636 - val_loss: 0.4067 - val_acc: 0.8750\n",
      "Epoch 75/100\n",
      "66/66 [==============================] - 0s 499us/step - loss: 0.3188 - acc: 0.8788 - val_loss: 0.4144 - val_acc: 0.7500\n",
      "Epoch 76/100\n",
      "66/66 [==============================] - 0s 544us/step - loss: 0.2969 - acc: 0.8939 - val_loss: 0.3973 - val_acc: 0.8750\n",
      "Epoch 77/100\n",
      "66/66 [==============================] - 0s 771us/step - loss: 0.3620 - acc: 0.8636 - val_loss: 0.4053 - val_acc: 0.7500\n",
      "Epoch 78/100\n",
      "66/66 [==============================] - 0s 514us/step - loss: 0.2983 - acc: 0.9091 - val_loss: 0.4319 - val_acc: 0.7500\n",
      "Epoch 79/100\n",
      "66/66 [==============================] - 0s 559us/step - loss: 0.2856 - acc: 0.8939 - val_loss: 0.4316 - val_acc: 0.7500\n",
      "Epoch 80/100\n",
      "66/66 [==============================] - 0s 499us/step - loss: 0.2801 - acc: 0.8939 - val_loss: 0.4478 - val_acc: 0.7500\n",
      "Epoch 81/100\n",
      "66/66 [==============================] - 0s 499us/step - loss: 0.2665 - acc: 0.9091 - val_loss: 0.4632 - val_acc: 0.7500\n",
      "Epoch 82/100\n",
      "66/66 [==============================] - 0s 499us/step - loss: 0.2555 - acc: 0.9091 - val_loss: 0.4567 - val_acc: 0.7500\n",
      "Epoch 83/100\n",
      "66/66 [==============================] - 0s 771us/step - loss: 0.2784 - acc: 0.9091 - val_loss: 0.4241 - val_acc: 0.8750\n",
      "Epoch 84/100\n",
      "66/66 [==============================] - 0s 574us/step - loss: 0.2662 - acc: 0.8939 - val_loss: 0.5706 - val_acc: 0.6250\n",
      "Epoch 85/100\n",
      "66/66 [==============================] - 0s 725us/step - loss: 0.2220 - acc: 0.9091 - val_loss: 0.4943 - val_acc: 0.6250\n",
      "Epoch 86/100\n",
      "66/66 [==============================] - 0s 771us/step - loss: 0.2332 - acc: 0.9091 - val_loss: 0.3325 - val_acc: 0.8750\n",
      "Epoch 87/100\n",
      "66/66 [==============================] - 0s 620us/step - loss: 0.2294 - acc: 0.9242 - val_loss: 0.4500 - val_acc: 0.7500\n",
      "Epoch 88/100\n",
      "66/66 [==============================] - 0s 529us/step - loss: 0.2175 - acc: 0.9394 - val_loss: 0.4705 - val_acc: 0.8750\n",
      "Epoch 89/100\n",
      "66/66 [==============================] - 0s 514us/step - loss: 0.2085 - acc: 0.9242 - val_loss: 0.4717 - val_acc: 0.8750\n",
      "Epoch 90/100\n",
      "66/66 [==============================] - 0s 801us/step - loss: 0.2390 - acc: 0.9242 - val_loss: 0.5200 - val_acc: 0.7500\n",
      "Epoch 91/100\n",
      "66/66 [==============================] - 0s 559us/step - loss: 0.1954 - acc: 0.9394 - val_loss: 0.3950 - val_acc: 0.8750\n",
      "Epoch 92/100\n",
      "66/66 [==============================] - 0s 786us/step - loss: 0.2024 - acc: 0.9242 - val_loss: 0.4691 - val_acc: 0.7500\n",
      "Epoch 93/100\n",
      "66/66 [==============================] - 0s 544us/step - loss: 0.1803 - acc: 0.9545 - val_loss: 0.3904 - val_acc: 0.8750\n",
      "Epoch 94/100\n",
      "66/66 [==============================] - 0s 574us/step - loss: 0.1790 - acc: 0.9091 - val_loss: 0.5296 - val_acc: 0.7500\n",
      "Epoch 95/100\n",
      "66/66 [==============================] - 0s 816us/step - loss: 0.1807 - acc: 0.8939 - val_loss: 0.4923 - val_acc: 0.7500\n",
      "Epoch 96/100\n",
      "66/66 [==============================] - 0s 801us/step - loss: 0.1609 - acc: 0.9394 - val_loss: 0.3953 - val_acc: 0.8750\n",
      "Epoch 97/100\n",
      "66/66 [==============================] - 0s 499us/step - loss: 0.1535 - acc: 0.9697 - val_loss: 0.4053 - val_acc: 0.8750\n",
      "Epoch 98/100\n",
      "66/66 [==============================] - 0s 816us/step - loss: 0.1547 - acc: 0.9545 - val_loss: 0.4200 - val_acc: 0.8750\n",
      "Epoch 99/100\n",
      "66/66 [==============================] - 0s 544us/step - loss: 0.1293 - acc: 0.9697 - val_loss: 0.4506 - val_acc: 0.8750\n",
      "Epoch 100/100\n",
      "66/66 [==============================] - 0s 529us/step - loss: 0.1429 - acc: 0.9545 - val_loss: 0.4289 - val_acc: 0.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2a114543ac8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KS11 = fdr.DataReader(\"KS11\",\"2010-01-01\",\"2019-01-01\")\n",
    "BATCH = 20\n",
    "\n",
    "df = KS11\n",
    "df=get_MA(df)\n",
    "df=get_stochastic(df)\n",
    "df=get_time(df)\n",
    "df=df[0:-(len(df)%BATCH)]\n",
    "\n",
    "MA_26=df[\"Close\"].rolling(26).mean()\n",
    "y_before=MA_26.dropna()\n",
    "df=df[25:]\n",
    "\n",
    "y=[]\n",
    "for a in range(1,10001):\n",
    "    try:\n",
    "        y.append(np.where(df[\"Close\"][-1+(BATCH*a)]>y_before[-1+(BATCH*a)],1,0)) # 19,39,59,...\n",
    "        #y.append(np.where(df[\"Close\"][-1+(BATCH*a)] > df[\"Close\"][-1+(BATCH*(a-1))],1,0))\n",
    "    except:\n",
    "        break\n",
    "\n",
    "df = df.values\n",
    "df.astype('float32')\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df = scaler.fit_transform(df)\n",
    "\n",
    "for x in range(100):\n",
    "Xtrain_size=int(df.shape[0]/BATCH*0.7)*BATCH + (x*BATCH)\n",
    "\n",
    "X_train=df[0:Xtrain_size]\n",
    "X_test=df[Xtrain_size:Xtrain_size+BATCH]\n",
    "\n",
    "y_train=y[ 0 : int(Xtrain_size/BATCH) ]\n",
    "y_test=y[int(Xtrain_size/BATCH)]\n",
    "\n",
    "# many to one 전처리 필요\n",
    "y_train = np.reshape(y_train, (len(y_train), 1))\n",
    "y_test = np.reshape(y_test, (1, 1))\n",
    "X_train = np.reshape(X_train, (int(X_train.shape[0]/BATCH),BATCH, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (int(X_test.shape[0]/BATCH),BATCH, X_test.shape[1]))\n",
    "\n",
    "\n",
    "X_train[:,BATCH-1,:]=0 #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "X_test[:,BATCH-1,:]=0 #!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "# simple lstm network learning\n",
    "model = Sequential()\n",
    "model.add(LSTM(36, input_shape=(BATCH, 12)))\n",
    "for i in range(5):\n",
    "    model.add(Dense(36,activation='sigmoid'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=16,validation_split=0.1,verbose=1)\n",
    "\n",
    "model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
